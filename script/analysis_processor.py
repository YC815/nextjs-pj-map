#!/usr/bin/env python3
"""
ç¨ç«‹çš„æª”æ¡ˆåˆ†æè™•ç†å™¨
åŠŸèƒ½ï¼šè®€å–cytoscape-elements.jsonï¼Œå¾GitHubç²å–æª”æ¡ˆå…§å®¹ï¼Œä½¿ç”¨OpenAI APIé€²è¡Œæ‘˜è¦å’ŒDockeré—œä¿‚åˆ†æ
æ”¯æ´å¾ç¶²è·¯ç²å–æˆ–å…‹éš†å€‰åº«åˆ°æœ¬åœ°å…©ç¨®æ¨¡å¼
"""

import json
import os
import time
import signal
import sys
import shutil
import subprocess
from typing import Dict, List, Optional, Any
from pathlib import Path
import asyncio
import aiohttp
from dataclasses import dataclass
from tqdm import tqdm
import openai
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field


# è¨­å®šä¸­æ–·è™•ç†
class GracefulKiller:
    kill_now = False

    def __init__(self):
        signal.signal(signal.SIGINT, self._exit_gracefully)
        signal.signal(signal.SIGTERM, self._exit_gracefully)

    def _exit_gracefully(self, *args):
        self.kill_now = True
        print("\nğŸ›‘ æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨å®‰å…¨é€€å‡º...")


@dataclass
class DockerAPI:
    apiType: str
    description: str
    line: Optional[int] = None
    codeSnippet: Optional[str] = None


class DockerAnalysisResult(BaseModel):
    """Docker åˆ†æçµæœçš„çµæ§‹åŒ–è¼¸å‡º"""
    hasDockerIntegration: bool = Field(description="æ˜¯å¦æœ‰Dockeræ•´åˆ")
    dockerApis: List[dict] = Field(description="Docker API åˆ—è¡¨", default_factory=list)
    dockerTools: List[str] = Field(description="Docker å·¥å…·åˆ—è¡¨", default_factory=list)
    summary: str = Field(description="æª”æ¡ˆæ‘˜è¦")
    fileType: str = Field(description="æª”æ¡ˆé¡å‹")
    keyFunctions: List[str] = Field(description="ä¸»è¦åŠŸèƒ½", default_factory=list)


class AnalysisProcessor:
    def __init__(
        self,
        openai_api_key: str,
        github_repo: str,
        github_token: Optional[str] = None,
        cache_dir: str = ".",
        use_local_clone: bool = False,
        clone_dir: str = "temp_repo_clone",
        branch: str = "main"
    ):
        self.openai_api_key = openai_api_key
        self.github_repo = github_repo
        self.github_token = github_token
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

        # æ–°å¢åƒæ•¸
        self.use_local_clone = use_local_clone
        self.clone_dir = Path(clone_dir)
        self.branch = branch
        self.cloned_successfully = False

        # åˆå§‹åŒ– OpenAI å’Œ LangChain
        self.llm = ChatOpenAI(
            api_key=openai_api_key,
            model="gpt-4o",
            temperature=0.1
        )

        # è¨­å®šè¼¸å‡ºè§£æå™¨
        self.output_parser = PydanticOutputParser(pydantic_object=DockerAnalysisResult)

        # å‰µå»ºæç¤ºæ¨¡æ¿
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç¨‹å¼ç¢¼åˆ†æå¸«ã€‚è«‹ä»”ç´°åˆ†æçµ¦å®šçš„ç¨‹å¼ç¢¼æª”æ¡ˆï¼Œæä¾›ä»¥ä¸‹è³‡è¨Šï¼š

1. æª”æ¡ˆæ‘˜è¦ï¼šç”¨ç¹é«”ä¸­æ–‡ç°¡è¦æè¿°æª”æ¡ˆçš„ä¸»è¦åŠŸèƒ½å’Œç”¨é€”ï¼Œç”¨ä¸€æ®µæ–‡å­—æè¿°çµ¦é–‹ç™¼è€…åƒè€ƒ
2. Docker æ•´åˆåˆ†æï¼š
   - æª¢æ¸¬æ˜¯å¦ä½¿ç”¨äº† Docker API æˆ–ç›¸é—œæŠ€è¡“
   - è­˜åˆ¥ Docker ç›¸é—œåŠŸèƒ½ï¼Œå¦‚å®¹å™¨ç®¡ç†ã€æ˜ åƒæª”æ“ä½œã€ç¶²è·¯é…ç½®ç­‰
   - åˆ†æä½¿ç”¨çš„ Docker å·¥å…·æˆ–å¥—ä»¶
   - è©•ä¼° Docker æ•´åˆçš„é‡è¦æ€§å’Œè¤‡é›œåº¦
3. æª”æ¡ˆé¡å‹åˆ†é¡ï¼ˆå¦‚ï¼šcomponent, api, utility, config, service ç­‰ï¼‰
4. ä¸»è¦åŠŸèƒ½åˆ—è¡¨

è«‹ç‰¹åˆ¥æ³¨æ„ä»¥ä¸‹ Docker ç›¸é—œé—œéµå­—ï¼š
- Docker API èª¿ç”¨
- å®¹å™¨æ“ä½œï¼ˆcreate, start, stop, removeï¼‰
- æ˜ åƒæª”æ“ä½œï¼ˆbuild, pull, pushï¼‰
- Docker Compose
- Dockerfile ç›¸é—œ
- å®¹å™¨ç·¨æ’
- Docker ç¶²è·¯å’Œå­˜å„²

{format_instructions}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œæ‘˜è¦éƒ¨åˆ†è¦è©³ç´°ä¸”å¯¦ç”¨ã€‚"""),
            ("user", """æª”æ¡ˆä½ç½®ï¼š{file_path}

æª”æ¡ˆå…§å®¹ï¼š
```
{file_content}
```

è«‹åˆ†æé€™å€‹æª”æ¡ˆä¸¦æä¾›çµæ§‹åŒ–çš„åˆ†æçµæœã€‚""")
        ])

        # ç·©å­˜æª”æ¡ˆè·¯å¾‘
        self.summaries_cache = self.cache_dir / "summaries.json"
        self.docker_analysis_cache = self.cache_dir / "docker-analysis.json"
        self.combined_cache = self.cache_dir / "combined-analysis.json"

        # è¼‰å…¥ç¾æœ‰ç·©å­˜
        self.summaries = self._load_cache(self.summaries_cache)
        self.docker_analysis = self._load_cache(self.docker_analysis_cache)
        self.combined_analysis = self._load_cache(self.combined_cache)

        # ä¸­æ–·è™•ç†
        self.killer = GracefulKiller()

    def _load_cache(self, cache_file: Path) -> Dict[str, Any]:
        """è¼‰å…¥ç·©å­˜æª”æ¡ˆ"""
        try:
            if cache_file.exists():
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"âš ï¸ è¼‰å…¥ç·©å­˜å¤±æ•— {cache_file}: {e}")
        return {}

    def _save_cache(self, cache_file: Path, data: Dict[str, Any]):
        """ä¿å­˜ç·©å­˜æª”æ¡ˆ"""
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ä¿å­˜ç·©å­˜å¤±æ•— {cache_file}: {e}")

    def _clone_repository(self) -> bool:
        """å…‹éš† GitHub å€‰åº«åˆ°æœ¬åœ°"""
        try:
            if self.clone_dir.exists():
                print(f"ğŸ—‘ï¸ åˆªé™¤ç¾æœ‰çš„å…‹éš†ç›®éŒ„: {self.clone_dir}")
                shutil.rmtree(self.clone_dir)

            github_url = f"https://github.com/{self.github_repo}.git"
            print(f"ğŸ“¦ æ­£åœ¨å…‹éš†å€‰åº«: {github_url}")
            print(f"ğŸŒ¿ åˆ†æ”¯: {self.branch}")

            # ä½¿ç”¨ git clone å‘½ä»¤
            result = subprocess.run([
                "git", "clone", "-b", self.branch, "--depth", "1",
                github_url, str(self.clone_dir)
            ], capture_output=True, text=True)

            if result.returncode == 0:
                print(f"âœ… å€‰åº«å…‹éš†æˆåŠŸåˆ°: {self.clone_dir}")
                self.cloned_successfully = True
                return True
            else:
                print(f"âŒ å…‹éš†å¤±æ•—: {result.stderr}")

                # å˜—è©¦å…¶ä»–å¸¸è¦‹åˆ†æ”¯åç¨±
                alternative_branches = ["master", "develop", "dev"]
                if self.branch == "main":
                    alternative_branches.insert(0, "master")
                elif self.branch == "master":
                    alternative_branches.insert(0, "main")

                for alt_branch in alternative_branches:
                    if alt_branch == self.branch:
                        continue

                    print(f"ğŸ”„ å˜—è©¦åˆ†æ”¯: {alt_branch}")
                    result = subprocess.run([
                        "git", "clone", "-b", alt_branch, "--depth", "1",
                        github_url, str(self.clone_dir)
                    ], capture_output=True, text=True)

                    if result.returncode == 0:
                        print(f"âœ… ä½¿ç”¨åˆ†æ”¯ {alt_branch} å…‹éš†æˆåŠŸ")
                        self.branch = alt_branch
                        self.cloned_successfully = True
                        return True

                return False

        except Exception as e:
            print(f"âŒ å…‹éš†å€‰åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

    def _read_local_file(self, file_path: str) -> Optional[str]:
        """å¾æœ¬åœ°å…‹éš†çš„å€‰åº«è®€å–æª”æ¡ˆï¼Œæ™ºæ…§åœ°å˜—è©¦ä¸åŒçš„è·¯å¾‘"""
        try:
            # å˜—è©¦çš„è·¯å¾‘é †åº
            possible_paths = [
                file_path,  # åŸå§‹è·¯å¾‘
                f"src/{file_path}",  # åŠ ä¸Š src/ å‰ç¶´
                f"./{file_path}",  # åŠ ä¸Š ./ å‰ç¶´
            ]

            for attempt_path in possible_paths:
                local_file_path = self.clone_dir / attempt_path
                if local_file_path.exists():
                    with open(local_file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    print(f"ğŸ“‚ å¾æœ¬åœ°è®€å–æª”æ¡ˆï¼š{attempt_path} (åŸè·¯å¾‘: {file_path})")
                    return content

            print(f"âš ï¸ æœ¬åœ°æª”æ¡ˆä¸å­˜åœ¨ï¼Œå·²å˜—è©¦è·¯å¾‘: {possible_paths}")
            return None

        except Exception as e:
            print(f"âŒ è®€å–æœ¬åœ°æª”æ¡ˆå¤±æ•— {file_path}: {e}")
            return None

    async def _fetch_github_file(self, session: aiohttp.ClientSession, file_path: str) -> Optional[str]:
        """å¾ GitHub ç²å–æª”æ¡ˆå…§å®¹ï¼ˆå„ªå…ˆä½¿ç”¨ Raw APIï¼Œæ›´å¿«ä¸”ç„¡éœ€èªè­‰ï¼‰"""

        # å¦‚æœä½¿ç”¨æœ¬åœ°å…‹éš†æ¨¡å¼
        if self.use_local_clone:
            if self.cloned_successfully:
                return self._read_local_file(file_path)
            else:
                print(f"âŒ å€‰åº«æœªæˆåŠŸå…‹éš†ï¼Œç„¡æ³•è®€å–æª”æ¡ˆ: {file_path}")
                return None

        # ç¶²è·¯æ¨¡å¼ - æ™ºæ…§è·¯å¾‘å˜—è©¦
        try:
            # å˜—è©¦ä¸åŒçš„åˆ†æ”¯
            branches_to_try = [self.branch]
            if self.branch == "main":
                branches_to_try.append("master")
            elif self.branch == "master":
                branches_to_try.append("main")

            # å˜—è©¦ä¸åŒçš„è·¯å¾‘å‰ç¶´
            path_variants = [
                file_path,  # åŸå§‹è·¯å¾‘
                f"src/{file_path}",  # åŠ ä¸Š src/ å‰ç¶´
                f"./{file_path}",  # åŠ ä¸Š ./ å‰ç¶´
            ]

            for branch in branches_to_try:
                for attempt_path in path_variants:
                    # å„ªå…ˆä½¿ç”¨ GitHub Raw API
                    raw_url = f"https://raw.githubusercontent.com/{self.github_repo}/{branch}/{attempt_path}"
                    async with session.get(raw_url) as response:
                        if response.status == 200:
                            content = await response.text()
                            print(f"ğŸ“¥ å¾ GitHub Raw ({branch}) ç²å–æª”æ¡ˆï¼š{attempt_path} (åŸè·¯å¾‘: {file_path})")
                            return content
                        elif response.status == 404:
                            continue  # å˜—è©¦ä¸‹ä¸€å€‹è·¯å¾‘/åˆ†æ”¯
                        else:
                            print(f"âŒ GitHub Raw API éŒ¯èª¤ {response.status} ({branch}): {attempt_path}")

            # å¦‚æœ Raw API éƒ½å¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨ GitHub API
            for branch in branches_to_try:
                for attempt_path in path_variants:
                    api_url = f"https://api.github.com/repos/{self.github_repo}/contents/{attempt_path}?ref={branch}"
                    headers = {}
                    if self.github_token:
                        headers["Authorization"] = f"token {self.github_token}"

                    async with session.get(api_url, headers=headers) as response:
                        if response.status == 200:
                            data = await response.json()
                            if data.get("type") == "file" and data.get("content"):
                                import base64
                                content = base64.b64decode(data["content"]).decode('utf-8')
                                print(f"ğŸ“¥ å¾ GitHub API ({branch}) ç²å–æª”æ¡ˆï¼š{attempt_path} (åŸè·¯å¾‘: {file_path})")
                                return content
                        elif response.status == 404:
                            continue  # å˜—è©¦ä¸‹ä¸€å€‹è·¯å¾‘/åˆ†æ”¯
                        else:
                            print(f"âŒ GitHub API éŒ¯èª¤ {response.status} ({branch}): {attempt_path}")

            print(f"âš ï¸ æ‰€æœ‰åˆ†æ”¯å’Œè·¯å¾‘éƒ½æ‰¾ä¸åˆ°æª”æ¡ˆ: {file_path}")

        except Exception as e:
            print(f"âŒ ç²å–æª”æ¡ˆå¤±æ•— {file_path}: {e}")
        return None

    async def _analyze_file(self, file_path: str, file_content: str) -> Optional[DockerAnalysisResult]:
        """ä½¿ç”¨ OpenAI åˆ†ææª”æ¡ˆ"""
        try:
            # æº–å‚™æç¤º
            formatted_prompt = self.prompt.format_messages(
                format_instructions=self.output_parser.get_format_instructions(),
                file_path=file_path,
                file_content=file_content[:8000]  # é™åˆ¶å…§å®¹é•·åº¦é¿å…è¶…å‡º token é™åˆ¶
            )

            # èª¿ç”¨ LLM
            response = await self.llm.ainvoke(formatted_prompt)

            # è§£æçµæœ
            try:
                result = self.output_parser.parse(response.content)
                if isinstance(result, DockerAnalysisResult):
                    return result
                else:
                    print(f"âš ï¸ è§£æçµæœä¸æ˜¯ DockerAnalysisResult é¡å‹ {file_path}: {type(result)}")
                    return None
            except Exception as parse_error:
                print(f"âš ï¸ è§£æ LLM å›æ‡‰å¤±æ•— {file_path}: {parse_error}")
                print(f"åŸå§‹å›æ‡‰å…§å®¹: {response.content[:500]}...")

                # å˜—è©¦å‰µå»ºä¸€å€‹åŸºæœ¬çš„åˆ†æçµæœ
                try:
                    basic_result = DockerAnalysisResult(
                        hasDockerIntegration=False,
                        dockerApis=[],
                        dockerTools=[],
                        summary=f"ç„¡æ³•å®Œæ•´åˆ†ææª”æ¡ˆ {file_path}ï¼Œä½†å·²æˆåŠŸè®€å–å…§å®¹ã€‚",
                        fileType="unknown",
                        keyFunctions=[]
                    )
                    return basic_result
                except Exception as fallback_error:
                    print(f"âŒ å‰µå»ºå‚™ç”¨åˆ†æçµæœå¤±æ•— {file_path}: {fallback_error}")
                    return None

        except Exception as e:
            print(f"âŒ åˆ†ææª”æ¡ˆå¤±æ•— {file_path}: {e}")
            return None

    def _load_cytoscape_elements(self, file_path: str = "./cytoscape-elements.json") -> List[str]:
        """è¼‰å…¥ cytoscape-elements.json ä¸¦æå–æª”æ¡ˆè·¯å¾‘"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                elements = json.load(f)

            file_paths = []
            for element in elements:
                if "data" in element and "id" in element["data"]:
                    element_id = element["data"]["id"]
                    # åªè™•ç†æª”æ¡ˆç¯€é»ï¼Œè·³éé€£ç·š
                    if "source" not in element["data"] and "target" not in element["data"]:
                        file_paths.append(element_id)

            print(f"ğŸ“ æ‰¾åˆ° {len(file_paths)} å€‹æª”æ¡ˆéœ€è¦åˆ†æ")
            return file_paths

        except Exception as e:
            print(f"âŒ è¼‰å…¥ cytoscape-elements.json å¤±æ•—: {e}")
            return []

    def cleanup(self):
        """æ¸…ç†è‡¨æ™‚æª”æ¡ˆ"""
        if self.use_local_clone and self.clone_dir.exists():
            try:
                print(f"ğŸ§¹ æ¸…ç†å…‹éš†ç›®éŒ„: {self.clone_dir}")
                shutil.rmtree(self.clone_dir)
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†å…‹éš†ç›®éŒ„å¤±æ•—: {e}")

    async def process_files(self, cytoscape_file: str = "./cytoscape-elements.json"):
        """è™•ç†æ‰€æœ‰æª”æ¡ˆ"""
        print("ğŸš€ é–‹å§‹æª”æ¡ˆåˆ†æè™•ç†...")
        print(f"ğŸ“¦ GitHub å€‰åº«: {self.github_repo}")
        print(f"ğŸŒ¿ åˆ†æ”¯: {self.branch}")
        print(f"ğŸ”§ æ¨¡å¼: {'æœ¬åœ°å…‹éš†' if self.use_local_clone else 'ç¶²è·¯ç²å–'}")

        # å¦‚æœä½¿ç”¨æœ¬åœ°å…‹éš†æ¨¡å¼ï¼Œå…ˆå…‹éš†å€‰åº«
        if self.use_local_clone:
            if not self._clone_repository():
                print("âŒ ç„¡æ³•å…‹éš†å€‰åº«ï¼Œè«‹æª¢æŸ¥å€‰åº«åç¨±å’Œç¶²è·¯é€£ç·š")
                return

        try:
            # è¼‰å…¥è¦è™•ç†çš„æª”æ¡ˆåˆ—è¡¨
            file_paths = self._load_cytoscape_elements(cytoscape_file)
            if not file_paths:
                print("âŒ æ²’æœ‰æ‰¾åˆ°è¦è™•ç†çš„æª”æ¡ˆ")
                return

            # éæ¿¾å·²è™•ç†çš„æª”æ¡ˆ
            pending_files = [f for f in file_paths if f not in self.combined_analysis]

            if not pending_files:
                print("âœ… æ‰€æœ‰æª”æ¡ˆéƒ½å·²åˆ†æå®Œæˆ")
                return

            print(f"ğŸ“‹ å¾…è™•ç†æª”æ¡ˆ: {len(pending_files)} å€‹")

            # å‰µå»ºé€²åº¦æ¢
            progress_bar = tqdm(
                total=len(pending_files),
                desc="åˆ†æé€²åº¦",
                unit="files",
                ncols=100
            )

            # å‰µå»º HTTP æœƒè©±ï¼ˆç¶²è·¯æ¨¡å¼æ‰éœ€è¦ï¼‰
            if not self.use_local_clone:
                timeout = aiohttp.ClientTimeout(total=30)
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    await self._process_files_with_session(session, pending_files, progress_bar)
            else:
                await self._process_files_with_session(None, pending_files, progress_bar)

            progress_bar.close()

            # æœ€çµ‚ä¿å­˜
            self._save_all_caches()

            print(f"\nâœ… åˆ†æå®Œæˆï¼å·²è™•ç† {len(self.combined_analysis)} å€‹æª”æ¡ˆ")
            self._print_statistics()

        finally:
            # æ¸…ç†
            if self.use_local_clone:
                self.cleanup()

    async def _process_files_with_session(self, session: Optional[aiohttp.ClientSession], pending_files: List[str], progress_bar):
        """ä½¿ç”¨æŒ‡å®šçš„æœƒè©±è™•ç†æª”æ¡ˆ"""
        for i, file_path in enumerate(pending_files):
            # æª¢æŸ¥ä¸­æ–·ä¿¡è™Ÿ
            if self.killer.kill_now:
                print("\nğŸ›‘ æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨ä¿å­˜é€²åº¦...")
                break

            progress_bar.set_description(f"è™•ç†: {file_path}")

            try:
                # ç²å–æª”æ¡ˆå…§å®¹
                if self.use_local_clone:
                    file_content = self._read_local_file(file_path)
                else:
                    file_content = await self._fetch_github_file(session, file_path)

                if not file_content:
                    progress_bar.update(1)
                    continue

                # åˆ†ææª”æ¡ˆ
                analysis_result = await self._analyze_file(file_path, file_content)
                if analysis_result:
                    try:
                        # ç¢ºä¿æ˜¯ DockerAnalysisResult çš„å¯¦ä¾‹
                        if isinstance(analysis_result, DockerAnalysisResult):
                            # ä¿å­˜åˆ°åˆä½µçš„åˆ†æçµæœ - ä½¿ç”¨ Pydantic çš„ model_dump æ–¹æ³•
                            self.combined_analysis[file_path] = analysis_result.model_dump()

                            # åŒæ™‚æ›´æ–°èˆŠçš„åˆ†é›¢ç·©å­˜ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
                            self.summaries[file_path] = analysis_result.summary

                            docker_analysis = {
                                "hasDockerIntegration": analysis_result.hasDockerIntegration,
                                "dockerApis": analysis_result.dockerApis,
                                "dockerTools": analysis_result.dockerTools,
                                "summary": analysis_result.summary
                            }
                            self.docker_analysis[file_path] = docker_analysis
                        else:
                            print(f"âš ï¸ åˆ†æçµæœé¡å‹éŒ¯èª¤ {file_path}: {type(analysis_result)}")
                    except Exception as e:
                        print(f"âš ï¸ ä¿å­˜åˆ†æçµæœå¤±æ•— {file_path}: {e}")
                        # å¦‚æœç„¡æ³•è½‰æ›ï¼Œç›´æ¥ä¿å­˜åŸå§‹çµæœ
                        self.combined_analysis[file_path] = str(analysis_result)

                # æ¯ 5 å€‹æª”æ¡ˆä¿å­˜ä¸€æ¬¡
                if (i + 1) % 5 == 0:
                    self._save_all_caches()

                progress_bar.update(1)

                # é¿å… API é™åˆ¶
                if not self.use_local_clone:
                    await asyncio.sleep(0.5)

            except Exception as e:
                print(f"\nâŒ è™•ç†æª”æ¡ˆå¤±æ•— {file_path}: {e}")
                progress_bar.update(1)
                continue

    def _save_all_caches(self):
        """ä¿å­˜æ‰€æœ‰ç·©å­˜"""
        self._save_cache(self.combined_cache, self.combined_analysis)
        self._save_cache(self.summaries_cache, self.summaries)
        self._save_cache(self.docker_analysis_cache, self.docker_analysis)

    def _print_statistics(self):
        """æ‰“å°çµ±è¨ˆè³‡è¨Š"""
        total_files = len(self.combined_analysis)
        docker_files = sum(1 for analysis in self.combined_analysis.values()
                           if analysis.get("hasDockerIntegration", False))

        print(f"\nğŸ“Š åˆ†æçµ±è¨ˆ:")
        print(f"   ç¸½æª”æ¡ˆæ•¸: {total_files}")
        print(f"   Docker æ•´åˆæª”æ¡ˆ: {docker_files}")
        print(f"   Docker æ•´åˆæ¯”ä¾‹: {docker_files / total_files * 100:.1f}%" if total_files > 0 else "")

        # æª”æ¡ˆé¡å‹çµ±è¨ˆ
        file_types = {}
        for analysis in self.combined_analysis.values():
            file_type = analysis.get("fileType", "unknown")
            file_types[file_type] = file_types.get(file_type, 0) + 1

        print(f"\nğŸ“‹ æª”æ¡ˆé¡å‹åˆ†ä½ˆ:")
        for file_type, count in sorted(file_types.items(), key=lambda x: x[1], reverse=True):
            print(f"   {file_type}: {count}")


def main():
    """ä¸»å‡½æ•¸"""
    # å¾ç’°å¢ƒè®Šæ•¸ç²å–é…ç½®
    openai_api_key = os.getenv("OPENAI_API_KEY")
    github_repo = os.getenv("GITHUB_REPO", "YC815/ai-web-ide")  # é è¨­ç‚ºæŒ‡å®šå€‰åº«
    github_token = os.getenv("GITHUB_TOKEN")  # å¯é¸çš„ GitHub token
    branch = os.getenv("GITHUB_BRANCH", "main")  # é è¨­åˆ†æ”¯
    use_local_clone = os.getenv("USE_LOCAL_CLONE", "false").lower() == "true"  # æ˜¯å¦ä½¿ç”¨æœ¬åœ°å…‹éš†

    if not openai_api_key:
        print("âŒ éŒ¯èª¤: è«‹è¨­å®š OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")
        sys.exit(1)

    print(f"ğŸ”‘ ä½¿ç”¨ GitHub å€‰åº«: {github_repo}")
    print(f"ğŸŒ¿ ä½¿ç”¨åˆ†æ”¯: {branch}")
    print(f"ğŸ”§ ä½¿ç”¨æ¨¡å¼: {'æœ¬åœ°å…‹éš†' if use_local_clone else 'ç¶²è·¯ç²å–'}")

    if github_token:
        print("ğŸ” ä½¿ç”¨ GitHub Token é€²è¡Œèªè­‰")
    else:
        print("ğŸ“– ä½¿ç”¨å…¬é–‹å€‰åº«ï¼Œç„¡ GitHub Token")

    # å‰µå»ºè™•ç†å™¨
    processor = AnalysisProcessor(
        openai_api_key=openai_api_key,
        github_repo=github_repo,
        github_token=github_token,
        use_local_clone=use_local_clone,
        branch=branch
    )

    # åŸ·è¡Œè™•ç†
    try:
        asyncio.run(processor.process_files())
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç¨‹åºè¢«ç”¨æˆ¶ä¸­æ–·")
        if use_local_clone:
            processor.cleanup()
    except Exception as e:
        print(f"\nâŒ ç¨‹åºåŸ·è¡Œå¤±æ•—: {e}")
        if use_local_clone:
            processor.cleanup()
        sys.exit(1)


if __name__ == "__main__":
    main()
